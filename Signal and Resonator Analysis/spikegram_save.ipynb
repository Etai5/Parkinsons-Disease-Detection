{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spikegram Save File  \n",
    "This file will load the offline resonator data created in file:'parkinson_full_signal_analysis_8_bands'  \n",
    "and create a spikegram from each file and save it to an output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def spikegram_from_spikes_event(spikes_event, clk_freq, window_ms=None, time_s=None):\n",
    "    ''' Create a Spectrogram from spikes event for a specified bins in miliseconds, default is 1 sec '''\n",
    "    samples = spikes_event.size\n",
    "    \n",
    "    if time_s == None:\n",
    "        time_s = samples // clk_freq # duration of signal in second rounded down\n",
    "        \n",
    "    if window_ms == None:\n",
    "        window_ms = 1000   # Window size of 1 second\n",
    "        \n",
    "    num_bins = (time_s * 1000) // window_ms\n",
    "    window_size = (clk_freq * window_ms) // 1000\n",
    "    spikes_spectrogram = np.zeros(num_bins)\n",
    "    for bin in range(num_bins):\n",
    "        spikes_spectrogram[bin] = np.sum(spikes_event[bin*window_size: (1+bin)*window_size+1])\n",
    "    \n",
    "    return spikes_spectrogram\n",
    "\n",
    "def create_spikegram(resonators_spikes_dict, clk_freq, window_ms=None, time_s=None):\n",
    "    spikegram = {}\n",
    "    time_bins = int(time_s) * 1000 // window_ms\n",
    "    for axis, resonators_spikes in resonators_spikes_dict.items():\n",
    "        spikegram[axis] = np.zeros((len(resonators_spikes), time_bins))   # for each axis: #resonators X #bins\n",
    "        for n, spikes_event in enumerate(resonators_spikes.values()):\n",
    "            spikegram[axis][n] = spikegram_from_spikes_event(spikes_event, clk_freq=clk_freq, window_ms=window_ms, time_s=int(time_s))\n",
    "            spikegram[axis][n] -= np.mean(spikegram[axis][n])\n",
    "            spikegram[axis][n][spikegram[axis][n] < 0] = 0\n",
    "    return spikegram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def align_fog_to_spikegram_high_sampling(fog_dict, spikegram_dict, spikegram_time_window_ms, clock_frequency):\n",
    "    # Access the first spikegram in spikegram_dict\n",
    "    first_key = list(spikegram_dict.keys())[0]  # Get the first key\n",
    "    first_spikegram = spikegram_dict[first_key]  # Access the first spikegram\n",
    "\n",
    "    # Get the number of time bins (x-axis length)\n",
    "    spikegram_length = first_spikegram.shape[1]\n",
    "    \n",
    "    spikegram_time_window_ms = 10  # Each bin in spikegram_dict is 10 ms\n",
    "    \n",
    "    # Sampling rate for fog_dict\n",
    "    clock_frequency = 15360\n",
    "    fog_bin_size = int(spikegram_time_window_ms / (1000 / clock_frequency))  # Corresponding samples per spikegram bin (1536 samples per bin)\n",
    "    \n",
    "    # Create new fog_dict aligned with spikegram_dict\n",
    "    aligned_fog_dict = {}\n",
    "    for event_type, fog_samples in fog_dict.items():\n",
    "        # Number of samples in fog_dict\n",
    "        num_fog_samples = len(fog_samples)\n",
    "        # Calculate the number of bins for fog_dict based on spikegram size\n",
    "        aligned_fog = []\n",
    "        \n",
    "        for i in range(0, num_fog_samples, fog_bin_size):\n",
    "            # Extract samples for this bin\n",
    "            bin_samples = fog_samples[i:i+fog_bin_size]\n",
    "            # Check if any sample in this bin is 1 (event occurred)\n",
    "            aligned_fog.append(1 if np.any(bin_samples) else 0)\n",
    "        \n",
    "        # Truncate or pad to match the spikegram size\n",
    "        aligned_fog = aligned_fog[:spikegram_length]  # Truncate to match spikegram size\n",
    "        if len(aligned_fog) < spikegram_length:\n",
    "            aligned_fog.extend([0] * (spikegram_length - len(aligned_fog)))  # Pad with 0s if shorter\n",
    "        \n",
    "        \n",
    "        aligned_fog_dict[event_type] = np.array(aligned_fog)\n",
    "    \n",
    "    return aligned_fog_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "def save_spikegrams(offline_directory, output_directory, spikegram_time_window_ms, clock_frequency):\n",
    "    # Create output directory to save there the spikegram offline\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    \n",
    "    # Iterate through all files in the directory\n",
    "    with tqdm(total=len(os.listdir(offline_directory))) as pbar:\n",
    "        for file in sorted(os.listdir(offline_directory)):\n",
    "            if file.endswith('.pkl'):  # Ensure it's a grouped pickle file\n",
    "                file_path = os.path.join(offline_directory, file)\n",
    "                file_name = os.path.basename(file).split('.')[0]  # Extract base name\n",
    "                \n",
    "                # Load the data\n",
    "                with open(file_path, 'rb') as f:\n",
    "                    dictionaries = pickle.load(f)\n",
    "                \n",
    "                # Load relevant data\n",
    "                resonators_spikes_dict = dictionaries['resonators_spikes_dict']\n",
    "                time_s = dictionaries['time_s']\n",
    "                fog_dict_r = dictionaries['fog_dict_r']\n",
    "                \n",
    "                # Create spikegram\n",
    "                spikegram = create_spikegram(resonators_spikes_dict=resonators_spikes_dict,\n",
    "                                            clk_freq=clock_frequency,\n",
    "                                            window_ms=spikegram_time_window_ms,\n",
    "                                            time_s=time_s)\n",
    "                \n",
    "                # Align FoG dict\n",
    "                spikegram_fog_dict = align_fog_to_spikegram_high_sampling(fog_dict=fog_dict_r,\n",
    "                                                                        spikegram_dict=spikegram,\n",
    "                                                                        spikegram_time_window_ms=spikegram_time_window_ms,\n",
    "                                                                        clock_frequency=clock_frequency)\n",
    "                \n",
    "                # Save each dictionary as a separate file\n",
    "                save_dict = {\n",
    "                    'spikegram_dict': spikegram,\n",
    "                    'spikegram_fog_dict': spikegram_fog_dict\n",
    "                }\n",
    "                \n",
    "                file_save_path = os.path.join(output_directory, f\"{file_name}_spikegram.pkl\")\n",
    "                with open(file_save_path, 'wb') as f:\n",
    "                    pickle.dump(save_dict, f)\n",
    "                    \n",
    "            pbar.update(1)   \n",
    "            \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 411/411 [07:51<00:00,  1.15s/it]\n"
     ]
    }
   ],
   "source": [
    "# folder_path = f'D:\\\\University\\\\project_data_files\\\\tdscfog_offline_data'\n",
    "# output_directory = 'D:\\\\University\\\\project_data_files\\\\tdscfog_spikegram_offline_data'\n",
    "spikegram_time_window_ms = 10\n",
    "clock_frequency = 15360\n",
    "'''\n",
    "save_spikegrams(offline_directory=folder_path,\n",
    "                output_directory=output_directory,\n",
    "                spikegram_time_window_ms=spikegram_time_window_ms,\n",
    "                clock_frequency=clock_frequency)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spikegram loading and verifying integrity of the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "def load_all_saved_data(offline_directory):\n",
    "    # Define the offline data directory\n",
    "    \n",
    "    if not os.path.exists(offline_directory):\n",
    "        raise FileNotFoundError(f\"The directory {offline_directory} does not exist.\")\n",
    "    \n",
    "    # Initialize a dictionary to store all loaded data\n",
    "    all_data = {}\n",
    "    \n",
    "    # Iterate through all grouped .pkl files in the directory\n",
    "    for file in sorted(os.listdir(offline_directory)):\n",
    "        if file.endswith('.pkl'):  # Ensure it's a grouped pickle file\n",
    "            file_path = os.path.join(offline_directory, file)\n",
    "            file_name = os.path.basename(file).split('.')[0]  # Extract base name\n",
    "            \n",
    "            # Load the grouped data\n",
    "            with open(file_path, 'rb') as f:\n",
    "                grouped_data = pickle.load(f)\n",
    "            \n",
    "            # Add to the dictionary under the file name\n",
    "            all_data[file_name] = grouped_data\n",
    "    \n",
    "    return all_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files Spikegrams and Labels of same size\n"
     ]
    }
   ],
   "source": [
    "folder_path = f'D:\\\\University\\\\tdscfog_spikegram_offline_data'\n",
    "all_data = load_all_saved_data(folder_path)\n",
    "flag = 0\n",
    "\n",
    "\n",
    "for index, (file_name, data1) in enumerate(all_data.items()):\n",
    "    spikegram_dict = data1['spikegram_dict']\n",
    "    spikegram_fog_dict = data1['spikegram_fog_dict']\n",
    "    for (spikegram_axis, spikegram_data), (fog_type, fog_event) in zip(spikegram_dict.items(), spikegram_fog_dict.items()):\n",
    "\n",
    "        # print(spikegram_data.shape[1])\n",
    "        # print(fog_event.shape[0])\n",
    "        \n",
    "        if spikegram_data.shape[1] != fog_event.shape[0]:\n",
    "            print(f'Not the same size')\n",
    "            flag = 1\n",
    "    \n",
    "if flag == 0:\n",
    "    print(f'All files Spikegrams and Labels of same size')\n",
    "            \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
